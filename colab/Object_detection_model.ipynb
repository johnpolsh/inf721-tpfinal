{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78CEeCESIMWb"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/johnpolsh/inf721-tpfinal/blob/main/colab/Object_detection_model.ipynb)\n",
        "## Setup\n",
        "### Download dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas matplotlib numpy wget zipfile36 torch==2.0.1 torchvision==0.15.2 torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* obs: use the following line if tring to run this on a local machine. Somethimes torch seems to be unable to detect the cuda device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch==2.0.1+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Default imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfznvUtUlvVN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import wget\n",
        "import zipfile\n",
        "\n",
        "# NOTE: if an cuda device is available, the line bellow will evaluate to '2.0.1+cu118'\n",
        "torch.__version__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fouOM7wxIMWg"
      },
      "source": [
        "### Select back-end device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaxflJNFIMWg"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_default_device(device)\n",
        "\n",
        "print(f\"Using {device} as default device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "### Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_zip_path = \"utensils.zip\"\n",
        "dataset_src_url = \"https://homepages.inf.ed.ac.uk/rbf/UTENSILS/raw.zip\"\n",
        "if not os.path.isfile(dataset_zip_path):\n",
        "    wget.download(dataset_src_url, dataset_zip_path)\n",
        "\n",
        "dataset_root_base_path = \"dataset/utensils\"\n",
        "if not os.path.isdir(dataset_root_base_path):\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_root_base_path)\n",
        "\n",
        "dataset_root_path = os.path.join(dataset_root_base_path, \"RAW IMAGES\")\n",
        "ls_root_path = \"\\\"\" + dataset_root_path + \"\\\"\"\n",
        "\n",
        "!ls $ls_root_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate dataset normalization mean/std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "our_dataset = ImageFolder(root=dataset_root_path)\n",
        "\n",
        "norm_mean = (0.,)\n",
        "norm_std = (0.,)\n",
        "dataset_len = len(our_dataset)\n",
        "make_tensor = transforms.ToTensor()\n",
        "for img, _ in our_dataset:\n",
        "    img = make_tensor(img).numpy().transpose((1, 2, 0))\n",
        "    w, h, c = img.shape\n",
        "    img = np.resize(img, (w * h, 3))\n",
        "    norm_mean += img.mean(0)\n",
        "    norm_std += img.std(0)\n",
        "\n",
        "norm_mean /= dataset_len\n",
        "norm_std /= dataset_len\n",
        "print(f\"Dataset normalization mean: {norm_mean}, std: {norm_std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Torch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(degrees=(0, 80)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(brightness=.4, contrast=.5, hue=.2),\n",
        "    transforms.GaussianBlur(kernel_size=(3, 7), sigma=(0.2, 4)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(norm_mean, norm_std)\n",
        "])\n",
        "our_dataset = ImageFolder(root=dataset_root_path, transform=transforms_train)\n",
        "\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(our_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
        "\n",
        "print(f\"Training dataset has {len(our_dataset)} examples\")\n",
        "print(f\"Training dataloader has {len(train_dataloader)} batches\")\n",
        "print(\"Using transforms:\")\n",
        "print(transforms_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "def sample():\n",
        "    img, lbl = our_dataset[randint(0, dataset_len - 1)]\n",
        "    img = np.array(img)\n",
        "    return (img, lbl)\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.subplot(2, 2, 1)\n",
        "img, lbl = sample()\n",
        "plt.imshow(img)\n",
        "plt.title(classes[lbl])\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "img, lbl = sample()\n",
        "plt.imshow(img)\n",
        "plt.title(classes[lbl])\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "img, lbl = sample()\n",
        "plt.imshow(img)\n",
        "plt.title(classes[lbl])\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "img, lbl = sample()\n",
        "plt.imshow(img)\n",
        "plt.title(classes[lbl])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "iterator = iter(train_dataloader)\n",
        "images, labels = next(iterator)\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "img_grid = make_grid(images)\n",
        "img = img_grid.numpy().transpose((1, 2, 0))\n",
        "img = norm_std * img + norm_mean\n",
        "img = np.clip(img, 0, 1)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "print([classes[labels[i]] for i in range(batch_size)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6_-eYrgIMWh"
      },
      "source": [
        "# Model\n",
        "### Our architecture definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJXY7cXEIMWh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "#dw\n",
        "class DepthWiseConvolution(nn.Sequential):\n",
        "    def __init__(self, in_fts, stride = 1):\n",
        "        super(DepthWiseConvolution,self).__init__(\n",
        "            nn.Conv2d(in_fts,in_fts,kernel_size=(3,3),stride=stride,padding=(1,1), groups=in_fts, bias=False),\n",
        "            nn.BatchNorm2d(in_fts),\n",
        "            nn.ReLU6(inplace=True))\n",
        "\n",
        "\n",
        "#pw\n",
        "class PointWiseConvolution(nn.Sequential):\n",
        "    def __init__(self,in_fts,out_fts):\n",
        "        super(PointWiseConvolution,self).__init__(\n",
        "            nn.Conv2d(in_fts,out_fts,kernel_size=(1,1),bias=False),\n",
        "            nn.BatchNorm2d(out_fts),\n",
        "            nn.ReLU6(inplace=True))\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1, norm_layer=None):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self,inp, oup, stride, expand_ratio, norm_layer=nn.BatchNorm2d):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        hidden_dim = int(round(inp*expand_ratio))\n",
        "        layers = []\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        #pw\n",
        "        if expand_ratio != 1:\n",
        "            layers.append(PointWiseConvolution(inp,hidden_dim))\n",
        "\n",
        "        #dw\n",
        "        layers.extend([\n",
        "            DepthWiseConvolution(hidden_dim,stride),\n",
        "            #pw-linear\n",
        "            nn.Conv2d(hidden_dim,oup,1,1,0,bias=False),\n",
        "            nn.BatchNorm2d(oup)])\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class OurObjectDetectionNet(nn.Module):\n",
        "    def __init__(self, bottleneckLayerDetail, inp = 3, num_classes=len(classes), width_mult=1.0, round_nearest=8):\n",
        "        super(OurObjectDetectionNet, self).__init__()\n",
        "\n",
        "        self.out = None\n",
        "\n",
        "        bloco = Bottleneck\n",
        "        inverted_residual_setting = bottleneckLayerDetail\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        input_channel = _make_divisible(input_channel*width_mult,round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel*width_mult,round_nearest)\n",
        "\n",
        "        #first layer\n",
        "        features = [ConvBNReLU(inp, input_channel, stride=2)]\n",
        "\n",
        "        #build layers\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c*width_mult,round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(bloco(input_channel,output_channel,stride = stride,expand_ratio=t))\n",
        "                input_channel = output_channel\n",
        "\n",
        "\n",
        "        #last layer\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
        "\n",
        "        #make sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        #classificador\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes))\n",
        "\n",
        "    def __forward_impl(self, x):\n",
        "        x = self.features(x)\n",
        "        x = nn.functional.adaptive_avg_pool2d(x,1).reshape(x.shape[0],-1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.__forward_impl(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhSGOJF5IT-j"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "bottleneckLayerDetail = [\n",
        "    # t, c, n, s\n",
        "    [1, 16, 1, 1],\n",
        "    [6, 24, 2, 2],\n",
        "    [6, 32, 3, 2],\n",
        "    [6, 64, 4, 2],\n",
        "    [6, 96, 3, 1],\n",
        "    [6, 160, 3, 2],\n",
        "    [6, 320, 1, 1],\n",
        "]\n",
        "\n",
        "our_model = OurObjectDetectionNet(bottleneckLayerDetail)\n",
        "summary(our_model, (1, 3, 224, 224), col_names=(\"input_size\", \"output_size\",\n",
        "                                                      \"num_params\", \"kernel_size\",\n",
        "                                                      \"mult_adds\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKkPP8ZMIMWi"
      },
      "source": [
        "### MobileNet V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8_CdcdxIMWi"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
        "\n",
        "mobilenet_model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(our_model.parameters(), lr=0.05)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_one_epoch(model, batch_i, loss_function, optim, sched=None):\n",
        "    model.train()\n",
        "    avg_loss = .0\n",
        "    run_loss = .0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    for i, (x_t, y_t) in enumerate(train_dataloader):\n",
        "        i += 1\n",
        "        x_t = x_t.to(device)\n",
        "        y_t = y_t.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            y_hat = nn.parallel.data_parallel(model, x_t)\n",
        "        else:\n",
        "            y_hat = model(x_t)\n",
        "\n",
        "        loss = loss_function(y_hat, y_t)\n",
        "        avg_loss += loss.item()\n",
        "        run_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(y_hat, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        targets.extend(y_t.cpu().numpy())\n",
        "\n",
        "        if i % batch_i == 0:\n",
        "            print(f\"\\tbatch {i} avg loss {run_loss / batch_i}\")\n",
        "            run_loss = .0\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    \n",
        "    if sched:\n",
        "        sched.step()\n",
        "\n",
        "    return (avg_loss / i, accuracy_score(targets, predictions))\n",
        "\n",
        "acc_history_train = []\n",
        "loss_history_train = []\n",
        "def train(n_epochs, model, model_save_dir, loss_function, optim, sched=None):\n",
        "    best_loss = 1_000_000\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"========================================================\\nEPOCH {epoch + 1}\")\n",
        "        avg_loss, accuracy = train_one_epoch(model, 5, loss_function, optim, sched)\n",
        "        loss_history_train.append(avg_loss)\n",
        "        acc_history_train.append(accuracy)\n",
        "        print(f\"avg train loss: {avg_loss}, train accuracy: {accuracy * 100:.2f}\")\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), model_save_dir)\n",
        "\n",
        "    print(f\"\\nTraining finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train(100, our_model, \"our_model.pth\", loss_fn, optimizer, scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving/loading model for resume training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model_for_resume(model, optim, path):\n",
        "    torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict()\n",
        "            }, path)\n",
        "\n",
        "def load_model_for_resume(path, model, optim):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optim.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_model_for_resume(our_model, optimizer, \"last-run.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_model_for_resume(\"last-run.pth\", our_model, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize model improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc_history_train)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accurary')\n",
        "plt.title('train accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss_history_train)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psncm1i2IMWi"
      },
      "source": [
        "### Convert model for mobile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBxTOOdcIMWj"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import os.path\n",
        "\n",
        "if not os.path.isfile(\"convert.py\"):\n",
        "    wget.download(\n",
        "        \"https://raw.githubusercontent.com/johnpolsh/inf721-tpfinal/main/colab/convert.py\",\n",
        "        \"convert.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YFid7LRIMWj"
      },
      "outputs": [],
      "source": [
        "from convert import convert_for_mobile\n",
        "\n",
        "convert_for_mobile(mobilenet_model, \"object_detection\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
