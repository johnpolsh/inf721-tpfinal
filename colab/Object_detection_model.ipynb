{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78CEeCESIMWb"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/johnpolsh/inf721-tpfinal/blob/main/colab/Object_detection_model.ipynb)\n",
        "## Setup\n",
        "### Download dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install wget fiftyone fiftyone-db==0.4.3 torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Default imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfznvUtUlvVN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import wget\n",
        "import fiftyone as fo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fouOM7wxIMWg"
      },
      "source": [
        "### Select back-end device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaxflJNFIMWg"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_default_device(device)\n",
        "\n",
        "print(f\"Using {device} as default device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = [\n",
        "        \"book\",\n",
        "        \"bottle\",\n",
        "        \"cup\",\n",
        "        \"bowl\",\n",
        "        \"knife\",\n",
        "        \"remote\",\n",
        "        \"vase\",\n",
        "        \"cell phone\",\n",
        "        \"spoon\",\n",
        "        \"laptop\",\n",
        "        \"fork\",\n",
        "        \"keyboard\",\n",
        "        \"mouse\"]\n",
        "dataset = fo.zoo.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"],\n",
        "    classes=labels\n",
        ")\n",
        "dataset.persistent = True\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "view = (\n",
        "    dataset\n",
        "    .filter_labels(\"detections\", F(\"label\").is_in(labels))\n",
        ")\n",
        "dataset.save_view(\"labels\", view)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export_dir = \"./dataset\"\n",
        "dataset_type = fo.types.YOLOv5Dataset\n",
        "label_fields = \"detections\"\n",
        "\n",
        "view.export(\n",
        "    export_dir=export_dir,\n",
        "    dataset_type=dataset_type,\n",
        "    label_field=label_fields\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Torch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional\n",
        "from torchvision.transforms.v2 import functional\n",
        "\n",
        "class CustomDetectionDataset(Dataset):\n",
        "    def __init__(self, images_folder, labels_folder, labels):\n",
        "        self.images_folder = images_folder\n",
        "        self.labels_folder = labels_folder\n",
        "        self.labels = labels\n",
        "\n",
        "        self.images_files = os.listdir(self.images_folder)\n",
        "        self.images_files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "        self.labels_files = os.listdir(self.labels_folder)\n",
        "        self.labels_files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "        \n",
        "        assert len(self.images_files) == len(self.labels_files)\n",
        "\n",
        "        self.__gen_labels_dict()\n",
        "        self.__gen_dataset_norm()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        def apply_transforms(img):\n",
        "            jitter = transforms.ColorJitter(brightness=.3, contrast=.3, hue=.1)\n",
        "            blur = transforms.GaussianBlur(kernel_size=(3, 13), sigma=(0.2, 6))\n",
        "            if np.random.rand() < 0.5:\n",
        "                img = functional.hflip(img)\n",
        "            if np.random.rand() < 0.5:\n",
        "                img = functional.vflip(img)\n",
        "            img = jitter(img)\n",
        "            img = blur(img)\n",
        "            img = functional.to_tensor(img)\n",
        "            img = functional.resize(img, (224, 224))\n",
        "            img = functional.normalize(img, self.norm_mean, self.norm_std)\n",
        "            return img\n",
        "\n",
        "        label = self.i2label[index]\n",
        "        image_file_path = os.path.join(self.images_folder, self.images_files[self.li2file[index]])\n",
        "        image = Image.open(image_file_path)\n",
        "        image = apply_transforms(image)\n",
        "\n",
        "        return (image, label)\n",
        "\n",
        "    def __gen_labels_dict(self):\n",
        "        self.dataset_size = 0\n",
        "        self.i2label = {}\n",
        "        self.li2file = {}\n",
        "        for i, label_file in enumerate(self.get_labels_paths()):\n",
        "            with open(label_file, 'r') as file:\n",
        "                for line in file:\n",
        "                    cls = re.search(r'\\d+', line.strip())\n",
        "                    if cls:\n",
        "                        self.i2label[self.dataset_size] = int(cls.group()) # dataset item idx has class cls\n",
        "                        self.li2file[self.dataset_size] = i # dataset item idx belongs to label_file i\n",
        "                        self.dataset_size += 1\n",
        "    \n",
        "    def __gen_dataset_norm(self):\n",
        "        self.norm_mean = (0.,)\n",
        "        self.norm_std = (0.,)\n",
        "        images = self.get_images_paths()\n",
        "        for img_file in images:\n",
        "            img = Image.open(img_file)\n",
        "            img = functional.to_tensor(img)\n",
        "            img = img.numpy().transpose((1, 2, 0))\n",
        "            w, h, c = img.shape\n",
        "            img = np.resize(img, (w * h, c))\n",
        "            self.norm_mean += img.mean(0)\n",
        "            self.norm_std += img.std(0)\n",
        "        self.norm_mean /= len(images)\n",
        "        self.norm_std /= len(images)\n",
        "\n",
        "    def get_label(self, index):\n",
        "        return self.labels[index]\n",
        "\n",
        "    def get_images_paths(self):\n",
        "        return [os.path.join(self.images_folder, self.images_files[i])\n",
        "                for i in range(len(self.images_files))]\n",
        "\n",
        "    def get_labels_paths(self):\n",
        "        return [os.path.join(self.labels_folder, self.labels_files[i])\n",
        "                for i in range(len(self.labels_files))]\n",
        "\n",
        "our_dataset = CustomDetectionDataset(\"/content/dataset/images/val\",\n",
        "                                     \"/content/dataset/labels/val\",\n",
        "                                     labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "def denorm(img):\n",
        "    img = img.transpose((1, 2, 0))\n",
        "    img = np.array(our_dataset.norm_std) * img + np.array(our_dataset.norm_mean)\n",
        "    return np.clip(img, 0, 1)\n",
        "\n",
        "def matplotlib_imshow(img):\n",
        "    if not isinstance(img, np.ndarray):\n",
        "        img = img.transpose((1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "\n",
        "print(f\"dataset size: {len(our_dataset)}\")\n",
        "print(f\"dataset normalization mean: {our_dataset.norm_mean}\")\n",
        "print(f\"dataset normalization std: {our_dataset.norm_std}\")\n",
        "image, label = our_dataset.__getitem__(randint(0, len(our_dataset)))\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "img = image.numpy()\n",
        "img = denorm(img)\n",
        "print(our_dataset.get_label(label))\n",
        "matplotlib_imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Torch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(our_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6_-eYrgIMWh"
      },
      "source": [
        "# Model\n",
        "### Our architecture definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJXY7cXEIMWh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "#dw\n",
        "class DepthWiseConvolution(nn.Sequential):\n",
        "    def __init__(self, in_fts, stride = 1):\n",
        "        super(DepthWiseConvolution,self).__init__(\n",
        "            nn.Conv2d(in_fts,in_fts,kernel_size=(3,3),stride=stride,padding=(1,1), groups=in_fts, bias=False),\n",
        "            nn.BatchNorm2d(in_fts),\n",
        "            nn.ReLU6(inplace=True))\n",
        "\n",
        "\n",
        "#pw\n",
        "class PointWiseConvolution(nn.Sequential):\n",
        "    def __init__(self,in_fts,out_fts):\n",
        "        super(PointWiseConvolution,self).__init__(\n",
        "            nn.Conv2d(in_fts,out_fts,kernel_size=(1,1),bias=False),\n",
        "            nn.BatchNorm2d(out_fts),\n",
        "            nn.ReLU6(inplace=True))\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1, norm_layer=None):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self,inp, oup, stride, expand_ratio, norm_layer=nn.BatchNorm2d):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        hidden_dim = int(round(inp*expand_ratio))\n",
        "        layers = []\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        #pw\n",
        "        if expand_ratio != 1:\n",
        "            layers.append(PointWiseConvolution(inp,hidden_dim))\n",
        "\n",
        "        #dw\n",
        "        layers.extend([\n",
        "            DepthWiseConvolution(hidden_dim,stride),\n",
        "            #pw-linear\n",
        "            nn.Conv2d(hidden_dim,oup,1,1,0,bias=False),\n",
        "            nn.BatchNorm2d(oup)])\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class OurObjectDetectionNet(nn.Module):\n",
        "    def __init__(self,bottleneckLayerDetail,inp = 3,num_classes = 50,width_mult = 1.0,round_nearest=8):\n",
        "        super(OurObjectDetectionNet, self).__init__()\n",
        "\n",
        "        self.out = None\n",
        "\n",
        "        bloco = Bottleneck\n",
        "        inverted_residual_setting = bottleneckLayerDetail\n",
        "\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        input_channel = _make_divisible(input_channel*width_mult,round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel*width_mult,round_nearest)\n",
        "\n",
        "        #first layer\n",
        "        features = [ConvBNReLU(inp, input_channel, stride=2)]\n",
        "\n",
        "        #build layers\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c*width_mult,round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(bloco(input_channel,output_channel,stride = stride,expand_ratio=t))\n",
        "                input_channel = output_channel\n",
        "\n",
        "\n",
        "        #last layer\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
        "\n",
        "        #make sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        #classificador\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes))\n",
        "\n",
        "    def __forward_impl(self, x):\n",
        "        x = self.features(x)\n",
        "        x = nn.functional.adaptive_avg_pool2d(x,1).reshape(x.shape[0],-1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.__forward_impl(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhSGOJF5IT-j"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "bottleneckLayerDetail = [\n",
        "    # t, c, n, s\n",
        "    [1, 16, 1, 1],\n",
        "    [6, 24, 2, 2],\n",
        "    [6, 32, 3, 2],\n",
        "    [6, 64, 4, 2],\n",
        "    [6, 96, 3, 1],\n",
        "    [6, 160, 3, 2],\n",
        "    [6, 320, 1, 1],\n",
        "]\n",
        "\n",
        "our_model = OurObjectDetectionNet(bottleneckLayerDetail)\n",
        "summary(our_model, (1, 3, 224, 224), col_names=(\"input_size\", \"output_size\",\n",
        "                                                      \"num_params\", \"kernel_size\",\n",
        "                                                      \"mult_adds\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKkPP8ZMIMWi"
      },
      "source": [
        "### MobileNet V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8_CdcdxIMWi"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
        "\n",
        "mobilenet_model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psncm1i2IMWi"
      },
      "source": [
        "### Convert model for mobile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lh4WLM6IMWj"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBxTOOdcIMWj"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import os.path\n",
        "\n",
        "if not os.path.isfile(\"convert.py\"):\n",
        "    wget.download(\n",
        "        \"https://raw.githubusercontent.com/johnpolsh/inf721-tpfinal/main/colab/convert.py\",\n",
        "        \"convert.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YFid7LRIMWj"
      },
      "outputs": [],
      "source": [
        "from convert import convert_for_mobile\n",
        "\n",
        "convert_for_mobile(mobilenet_model, \"object_detection\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
